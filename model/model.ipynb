{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping, found downloaded files in \"./garbage-classification\" (use force=True to force download)\n"
     ]
    }
   ],
   "source": [
    "import opendatasets as od\n",
    "\n",
    "\n",
    "od.download(\"https://www.kaggle.com/datasets/mostafaabla/garbage-classification\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/coldstuff1/codejam/CodeJam13/model/garbage-classification\n"
     ]
    }
   ],
   "source": [
    "import pathlib\n",
    "dataPath = pathlib.Path.cwd() / \"garbage-classification\"\n",
    "\n",
    "print(dataPath)\n",
    "\n",
    "isFile = lambda path : path.is_file()\n",
    "\n",
    "imagePaths = list(filter(isFile, list(dataPath.rglob(\"*\"))))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python Platform: Linux-6.5.9-arch2-1-x86_64-with-glibc2.38\n",
      "PyTorch Version: 2.1.1+cu121\n",
      "\n",
      "Python 3.11.0 (main, Mar  1 2023, 18:26:19) [GCC 11.2.0]\n",
      "NVIDIA/CUDA GPU is available\n",
      "MPS (Apple Metal) is NOT AzVAILABLE\n",
      "Target device is cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import sys\n",
    "import platform\n",
    "has_gpu = torch.cuda.is_available()\n",
    "# has_mps = getattr(torch,'has_mps',False)\n",
    "has_mps = torch.backends.mps.is_built()\n",
    "device = \"mps\" if torch.backends.mps.is_built() \\\n",
    "    else \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "print(f\"Python Platform: {platform.platform()}\")\n",
    "print(f\"PyTorch Version: {torch.__version__}\")\n",
    "print()\n",
    "print(f\"Python {sys.version}\")\n",
    "print(\"NVIDIA/CUDA GPU is\", \"available\" if has_gpu else \"NOT AVAILABLE\")\n",
    "print(\"MPS (Apple Metal) is\", \"AVAILABLE\" if has_mps else \"NOT AzVAILABLE\")\n",
    "print(f\"Target device is {device}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 2.84 GiB. GPU 0 has a total capacty of 5.62 GiB of which 2.51 GiB is free. Including non-PyTorch memory, this process has 3.11 GiB memory in use. Of the allocated memory 2.84 GiB is allocated by PyTorch, and 193.04 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "\u001b[1;32m/home/coldstuff1/codejam/CodeJam13/model/model.ipynb Cell 4\u001b[0m line \u001b[0;36m1\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/coldstuff1/codejam/CodeJam13/model/model.ipynb#W4sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m     img \u001b[39m=\u001b[39m Image\u001b[39m.\u001b[39mopen(\u001b[39mstr\u001b[39m(path))\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/coldstuff1/codejam/CodeJam13/model/model.ipynb#W4sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m     inputImages\u001b[39m.\u001b[39mappend(transformer(img)\u001b[39m.\u001b[39mto(device))\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/coldstuff1/codejam/CodeJam13/model/model.ipynb#W4sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m dataset \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mcat(inputImages)\u001b[39m.\u001b[39mto(device)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/coldstuff1/codejam/CodeJam13/model/model.ipynb#W4sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m \u001b[39mprint\u001b[39m(dataset\u001b[39m.\u001b[39mshape)\n",
      "File \u001b[0;32m~/anaconda3/envs/codejam/lib/python3.11/site-packages/torchvision/tv_tensors/_tv_tensor.py:77\u001b[0m, in \u001b[0;36mTVTensor.__torch_function__\u001b[0;34m(cls, func, types, args, kwargs)\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[39m# Like in the base Tensor.__torch_function__ implementation, it's easier to always use\u001b[39;00m\n\u001b[1;32m     75\u001b[0m \u001b[39m# DisableTorchFunctionSubclass and then manually re-wrap the output if necessary\u001b[39;00m\n\u001b[1;32m     76\u001b[0m \u001b[39mwith\u001b[39;00m DisableTorchFunctionSubclass():\n\u001b[0;32m---> 77\u001b[0m     output \u001b[39m=\u001b[39m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs \u001b[39mor\u001b[39;49;00m \u001b[39mdict\u001b[39;49m())\n\u001b[1;32m     79\u001b[0m must_return_subclass \u001b[39m=\u001b[39m _must_return_subclass()\n\u001b[1;32m     80\u001b[0m \u001b[39mif\u001b[39;00m must_return_subclass \u001b[39mor\u001b[39;00m (func \u001b[39min\u001b[39;00m _FORCE_TORCHFUNCTION_SUBCLASS \u001b[39mand\u001b[39;00m \u001b[39misinstance\u001b[39m(args[\u001b[39m0\u001b[39m], \u001b[39mcls\u001b[39m)):\n\u001b[1;32m     81\u001b[0m     \u001b[39m# If you're wondering why we need the `isinstance(args[0], cls)` check, remove it and see what fails\u001b[39;00m\n\u001b[1;32m     82\u001b[0m     \u001b[39m# in test_to_tv_tensor_reference().\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     86\u001b[0m     \u001b[39m# `args = (a_pure_tensor, an_image)` first. Without this guard, `out` would\u001b[39;00m\n\u001b[1;32m     87\u001b[0m     \u001b[39m# be wrapped into an `Image`.\u001b[39;00m\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 2.84 GiB. GPU 0 has a total capacty of 5.62 GiB of which 2.51 GiB is free. Including non-PyTorch memory, this process has 3.11 GiB memory in use. Of the allocated memory 2.84 GiB is allocated by PyTorch, and 193.04 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "import torchvision.transforms.v2 as v2\n",
    "\n",
    "transformer = v2.Compose([\n",
    "    v2.ToImage(),\n",
    "    v2.ToDtype(torch.uint8, scale=True),\n",
    "    v2.RandomResizedCrop(size=(256, 256), antialias=True)\n",
    "])\n",
    "\n",
    "inputImages = []\n",
    "\n",
    "for path in imagePaths:\n",
    "    img = Image.open(str(path))\n",
    "    inputImages.append(transformer(img).to(device))\n",
    "\n",
    "dataset = torch.cat(inputImages).to(device)\n",
    "\n",
    "print(dataset.shape)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "codejam",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
